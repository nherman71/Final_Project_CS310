{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T04:31:22.682924Z",
     "start_time": "2024-12-03T04:31:10.471623Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from keras.utils import image_dataset_from_directory\n",
    "#from examples.dataset.write_dataset_encrypted import dataset, pformat"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:35:52.933894Z",
     "start_time": "2024-12-03T04:35:42.497776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/pcuenq/oxford-pets/data/train-00000-of-00001-ecc2afb43dedd5e0.parquet\")"
   ],
   "id": "a2320c5f5b5eb0c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:35:58.610434Z",
     "start_time": "2024-12-03T04:35:58.583680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.size)\n",
    "images = [img['bytes'] for img in [3]]\n",
    "labels = df['label'].values\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image_new = tf.image.decode_jpeg(image, channels=3)\n",
    "    image_new = tf.image.resize(image_new, IMG_SIZE)\n",
    "    image_new = image_new / 255.0\n",
    "    return image_new, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"here\")\n",
    "print(df.head)"
   ],
   "id": "ebf8bd9bc21367f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29560\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39msize)\n\u001B[1;32m----> 2\u001B[0m images \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbytes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      3\u001B[0m labels \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m      5\u001B[0m IMG_SIZE \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39msize)\n\u001B[1;32m----> 2\u001B[0m images \u001B[38;5;241m=\u001B[39m [\u001B[43mimg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbytes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m3\u001B[39m]]\n\u001B[0;32m      3\u001B[0m labels \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m      5\u001B[0m IMG_SIZE \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_URL = 'https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz'\n",
    "import kagglehub\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "#path_to_zip = tf.keras.utils.get_file('cats_and_dogs.tar.gz', origin=_URL, extract=True, untar=True)\n",
    "import os\n",
    "#dir_path = os.path.dirname(os.path.realpath('images.tar.gz'))\n",
    "\n",
    "print(PATH)\n",
    "train_dir = os.path.join(PATH, 'images')\n",
    "validation_dir = os.path.join(PATH, '1')\n",
    "dataset = load_dataset(\"pcuenq/oxford-pets\", cache_dir=\"dogs_and_cats.csv\")\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "#test = tf.keras.utils.text_dataset_from_directory(PATH)\n",
    "path = dataset.cache_files['filename']\n",
    "PATH = os.path.join(os.path.dirname(str(path)))\n",
    "print(path)\n",
    "img_dataset = tf.keras.utils.image_dataset_from_directory(PATH, shuffle=True, image_size= IMG_SIZE, seed=4, validation_split=0.2, subset='both')\n",
    "\n",
    "train_dataset = img_dataset[0]\n",
    "validation_dataset = img_dataset[1]\n"
   ],
   "id": "76735868a4e4565e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(PATH,\n",
    "                                                                 shuffle=True,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 image_size=IMG_SIZE)"
   ],
   "id": "831417cc178d4f69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "id": "32ebda70b7cdb4dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ],
   "id": "268b573ca5a1b4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ],
   "id": "c9407bb3943759af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ],
   "id": "c2ef8756b15f344d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])"
   ],
   "id": "f47f7e4f06ff5a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ],
   "id": "9921186061ce595d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input",
   "id": "4b6fdd5c857a764c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)",
   "id": "4a4bdcef1506f4ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ],
   "id": "26aa5a3e7611678c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ],
   "id": "37968eec5a4528de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.trainable = False",
   "id": "63683e743fe7e760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ],
   "id": "22c2add6d1995227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ],
   "id": "6b4ee10452ffafea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ],
   "id": "2110f3573c6f80e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ],
   "id": "68a28dbb0fdbbdc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "347949df25828205",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(model.trainable_variables)",
   "id": "ca93ba0f7610f862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tf.keras.utils.plot_model(model, show_shapes=True)",
   "id": "1b577d43c73dde5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ],
   "id": "29e5ba316888041a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ],
   "id": "c38ab7a8e082176a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ],
   "id": "abe5825cde0e86c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ],
   "id": "523605c73f573bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "id": "49ac7bdcc609b965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.trainable = True",
   "id": "17f5b2f1451ba1dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ],
   "id": "f86b914a2f6cf003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ],
   "id": "e3482f4f6d09c76b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "c1e33b1afb62d284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(model.trainable_variables)",
   "id": "153057bc7af2072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=len(history.epoch),\n",
    "                         validation_data=validation_dataset)"
   ],
   "id": "7069a95c5e11b85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ],
   "id": "bfe855f24d981465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "id": "6cc165f1c30ccbb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ],
   "id": "737028873b8a1876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "    plt.title(class_names[predictions[i]])\n",
    "    plt.axis(\"off\")"
   ],
   "id": "4692ee501cb621b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "30d55860b9d91e26",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
