{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:13.431551Z",
     "start_time": "2024-11-21T16:58:13.426498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from sympy.strategies.core import switch\n",
    "\n",
    "print(sys.executable)"
   ],
   "id": "7c90fa97080305bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:22.412956Z",
     "start_time": "2024-11-21T16:58:14.510941Z"
    }
   },
   "source": [
    "# first install required dependencies with:\n",
    "# pip install torch torchvision transformers datasets\n",
    "# Then load the model from huggingface\n",
    "import transformers\n",
    "\n",
    "from transformers import MobileNetV2ForImageClassification, AutoImageProcessor\n",
    "\n",
    "# Load the model and image processor\n",
    "model_name = \"google/mobilenet_v2_1.0_224\"\n",
    "model = MobileNetV2ForImageClassification.from_pretrained(model_name)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:26.530117Z",
     "start_time": "2024-11-21T16:58:26.281367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Test on an image of a car\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "#Load image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/BlkStdSchnauzer2.jpg/440px-BlkStdSchnauzer2.jpg\"  \n",
    "# Replace with any sample image URL\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "#pass raw values to model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "#normalize outputs to a probability distribution\n",
    "probabilities = softmax(logits, dim=1)\n",
    "num_predictions = 5\n",
    "top_probs, top_classes = probabilities.topk(num_predictions, dim=1)\n",
    "# This will print out the post-softmax values along with their class labels (still unintelligible to humans)\n",
    "print(\"Top probabilities:\", top_probs)\n",
    "print(\"Top classes:\", top_classes)"
   ],
   "id": "cc19e6222b2b8f3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top probabilities: tensor([[0.6593, 0.1369, 0.0238, 0.0233, 0.0137]], grad_fn=<TopkBackward0>)\n",
      "Top classes: tensor([[199, 198, 263, 197, 200]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:28.946926Z",
     "start_time": "2024-11-21T16:58:28.941608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#now labeling the outputs, we can read what the predictions are\n",
    "labels = model.config.id2label\n",
    "for i in range(num_predictions):\n",
    "    class_id = top_classes[0][i].item()\n",
    "    label = labels.get(class_id, \"Unknown\")\n",
    "    probability = top_probs[0][i].item()\n",
    "    print(f\"Class: {label}, Probability: {probability:.4f}\")"
   ],
   "id": "d07ce405e90091ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: standard schnauzer, Probability: 0.6593\n",
      "Class: giant schnauzer, Probability: 0.1369\n",
      "Class: Brabancon griffon, Probability: 0.0238\n",
      "Class: miniature schnauzer, Probability: 0.0233\n",
      "Class: Scotch terrier, Scottish terrier, Scottie, Probability: 0.0137\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we seek to get a sense of the model and data and make needed changes",
   "id": "544251cf56185cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:31.809623Z",
     "start_time": "2024-11-21T16:58:31.804576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#first we need to get a sense of the model\n",
    "print(model)\n",
    "#the model is made up of many 3x3 convolution layers that are then reduced and eventually coded to categorize them"
   ],
   "id": "1f10cf32dc61246d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2ForImageClassification(\n",
      "  (mobilenet_v2): MobileNetV2Model(\n",
      "    (conv_stem): MobileNetV2Stem(\n",
      "      (first_conv): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6()\n",
      "      )\n",
      "      (conv_3x3): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "        (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6()\n",
      "      )\n",
      "      (reduce_1x1): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (normalization): BatchNorm2d(16, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer): ModuleList(\n",
      "      (0): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(24, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(24, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3-4): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(64, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6-8): 3 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(64, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10-11): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), groups=576, bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(160, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13-14): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(160, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(320, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_1x1): MobileNetV2ConvLayer(\n",
      "      (convolution): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(1280, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (classifier): Linear(in_features=1280, out_features=1001, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:16:53.848078Z",
     "start_time": "2024-11-21T17:16:53.316458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this block will help us get a sense of the data we are working with.\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pcuenq/oxford-pets\")\n",
    "print(dataset)\n",
    "# Now printing a single element\n",
    "print(dataset['train'][0]['image']['bytes'])"
   ],
   "id": "e596174a4276e842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['path', 'label', 'dog', 'image'],\n",
      "        num_rows: 7390\n",
      "    })\n",
      "})\n",
      "191\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:20:20.126334Z",
     "start_time": "2024-11-21T17:20:20.118862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#as the structure of the data is as raw bytes the easiest way to check the sizing is to change it into another format\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# we will look at 5 images to get a better sense of the data, as mobilenet v2 only accepts 224x224 images\n",
    "num_check = 5\n",
    "for i in range(num_check):\n",
    "    image_raw_test = dataset['train'][i]['image']['bytes']\n",
    "    image_conv = Image.open(BytesIO(image_raw_test))\n",
    "    print(f\"Image size: {image_conv.size}\")"
   ],
   "id": "fefb1bb9fc3fb520",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (345, 500)\n",
      "Image size: (290, 370)\n",
      "Image size: (333, 500)\n",
      "Image size: (500, 375)\n",
      "Image size: (416, 500)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:10:39.375653Z",
     "start_time": "2024-11-21T17:10:37.988216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#as the data is all different sizes we continue by resizing all images in our dataset to comply with the pre-trained model\n",
    "dataset_size = 7390\n",
    "def resize(datapoint):\n",
    "    image_raw = datapoint['image']['bytes']\n",
    "    image_workable = Image.open(BytesIO(image_raw)).convert('RGB')\n",
    "    image_workable = image_workable.resize((224, 224))\n",
    "    # Apply feature extractor to get the pixel values for the model\n",
    "    datapoint['image']['bytes'] = image_processor(images=image_workable, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return datapoint\n",
    "\n",
    "dataset_processed = dataset.map(resize, batched=False)"
   ],
   "id": "7f36f6135adaff14",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:37:38.334032Z",
     "start_time": "2024-11-21T17:37:38.325466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#noticing that the dataset is not split we do so making sure to keep a consistent random seed for reproducibility\n",
    "dataset_split = dataset_processed['train'].train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "# Access the splits\n",
    "train_data = dataset_split[\"train\"]\n",
    "test_data = dataset_split[\"test\"]\n",
    "print(train_data)\n",
    "print(test_data)"
   ],
   "id": "d8e9357d9343d552",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'label', 'dog', 'image'],\n",
      "    num_rows: 5912\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'label', 'dog', 'image'],\n",
      "    num_rows: 1478\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we begin the process of fine-tuning via transfer learning",
   "id": "54abf570421f29d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "#now we restructure the model to have 37 classes to match the pets dataset\n",
    "num_classes = 37\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)"
   ],
   "id": "c64f2213fdb1ad2c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
