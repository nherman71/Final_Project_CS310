{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:25:25.686642Z",
     "start_time": "2024-11-21T16:25:25.681041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ],
   "id": "7c90fa97080305bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T16:25:38.368542Z",
     "start_time": "2024-11-21T16:25:29.599379Z"
    }
   },
   "source": [
    "# first install required dependencies with:\n",
    "# pip install torch torchvision transformers datasets\n",
    "# Then load the model from huggingface\n",
    "import transformers\n",
    "\n",
    "from transformers import MobileNetV2ForImageClassification, AutoImageProcessor\n",
    "\n",
    "# Load the model and image processor\n",
    "model_name = \"google/mobilenet_v2_1.0_224\"\n",
    "model = MobileNetV2ForImageClassification.from_pretrained(model_name)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Test on an image of a car\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "#Load image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/BlkStdSchnauzer2.jpg/440px-BlkStdSchnauzer2.jpg\"  \n",
    "# Replace with any sample image URL\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "#pass raw values to model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "#normalize outputs to a probability distribution\n",
    "probabilities = softmax(logits, dim=1)\n",
    "num_predictions = 5\n",
    "top_probs, top_classes = probabilities.topk(num_predictions, dim=1)\n",
    "# This will print out the post-softmax values along with their class labels (still unintelligible to humans)\n",
    "print(\"Top probabilities:\", top_probs)\n",
    "print(\"Top classes:\", top_classes)"
   ],
   "id": "cc19e6222b2b8f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#now labeling the outputs, we can read what the predictions are\n",
    "labels = model.config.id2label\n",
    "for i in range(num_predictions):\n",
    "    class_id = top_classes[0][i].item()\n",
    "    label = labels.get(class_id, \"Unknown\")\n",
    "    probability = top_probs[0][i].item()\n",
    "    print(f\"Class: {label}, Probability: {probability:.4f}\")"
   ],
   "id": "d07ce405e90091ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we seek to get a sense of the model and data and make needed changes",
   "id": "544251cf56185cca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#first we need to get a sense of the model\n",
    "print(model)\n",
    "#the model is made up of many 3x3 convolution layers that are then reduced and eventually coded to categorize them"
   ],
   "id": "1f10cf32dc61246d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this block will help us get a sense of the data we are working with.\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pcuenq/oxford-pets\")\n",
    "print(dataset)\n",
    "# Now printing a single element\n",
    "print(dataset['train'][0])"
   ],
   "id": "e596174a4276e842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#as the structure of the data is as raw bytes the easiest way to check the sizing is to change it into another format\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# we will look at 5 images to get a better sense of the data, as mobilenet v2 only accepts 224x224 images\n",
    "num_check = 5\n",
    "for i in range(num_check):\n",
    "    image_raw_test = dataset['train'][i]['image']['bytes']\n",
    "    image_conv = Image.open(BytesIO(image_raw_test))\n",
    "    print(f\"Image size: {image_conv.size}\")"
   ],
   "id": "fefb1bb9fc3fb520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#as the data is all different sizes we continue by resizing all images in our dataset to comply with the pre-trained model\n",
    "dataset_size = 7390\n",
    "dataset_processed = dataset\n",
    "for i in range(dataset_size):\n",
    "    if (i+1)%739 == 0:\n",
    "        print(i)\n",
    "    image_raw = dataset['train'][i]['image']['bytes']\n",
    "    image_workable = Image.open(BytesIO(image_raw)).convert('RGB')\n",
    "    image_workable = image_workable.resize((224, 224))\n",
    "    # Apply feature extractor to get the pixel values for the model\n",
    "    dataset_processed['train'][i]['image']['bytes'] = image_processor(images=image_workable, return_tensors=\"pt\")[\"pixel_values\"]"
   ],
   "id": "7f36f6135adaff14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#noticing that the dataset is not split we do so making sure to keep a consistent random seed for reproducibility\n",
    "dataset_split = dataset_processed.train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "# Access the splits\n",
    "train_data = dataset_split[\"train\"]\n",
    "test_data = dataset_split[\"test\"]\n",
    "print(train_data)\n",
    "print(test_data)"
   ],
   "id": "c7387b379382d211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(dataset_processed)",
   "id": "e01521160b91c1da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we will look again at the first five images to see if they are the correct size\n",
    "num_check = 5\n",
    "for i in range(num_check):\n",
    "    image_raw_test = dataset_processed['train'][i]['image']['bytes']\n",
    "    image_conv = Image.open(BytesIO(image_raw_test))\n",
    "    print(f\"Image size: {image_conv.size}\")"
   ],
   "id": "cf73199137fa53ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we begin the process of fine-tuning via transfer learning",
   "id": "54abf570421f29d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "#now we restructure the model to have 37 classes to match the pets dataset\n",
    "num_classes = 37\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)"
   ],
   "id": "c64f2213fdb1ad2c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
