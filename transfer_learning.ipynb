{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:13.431551Z",
     "start_time": "2024-11-21T16:58:13.426498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from sympy.strategies.core import switch\n",
    "\n",
    "print(sys.executable)"
   ],
   "id": "7c90fa97080305bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game_\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T17:58:50.502638Z",
     "start_time": "2024-11-21T17:58:50.283013Z"
    }
   },
   "source": [
    "# first install required dependencies with:\n",
    "# pip install torch torchvision transformers datasets\n",
    "# Then load the model from huggingface\n",
    "import transformers\n",
    "\n",
    "from transformers import MobileNetV2ForImageClassification, AutoImageProcessor\n",
    "\n",
    "# Load the model and image processor\n",
    "model_name = \"google/mobilenet_v2_1.0_224\"\n",
    "model = MobileNetV2ForImageClassification.from_pretrained(model_name)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:26.530117Z",
     "start_time": "2024-11-21T16:58:26.281367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Test on an image of a car\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "#Load image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/BlkStdSchnauzer2.jpg/440px-BlkStdSchnauzer2.jpg\"  \n",
    "# Replace with any sample image URL\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "#pass raw values to model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "#normalize outputs to a probability distribution\n",
    "probabilities = softmax(logits, dim=1)\n",
    "num_predictions = 5\n",
    "top_probs, top_classes = probabilities.topk(num_predictions, dim=1)\n",
    "# This will print out the post-softmax values along with their class labels (still unintelligible to humans)\n",
    "print(\"Top probabilities:\", top_probs)\n",
    "print(\"Top classes:\", top_classes)"
   ],
   "id": "cc19e6222b2b8f3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top probabilities: tensor([[0.6593, 0.1369, 0.0238, 0.0233, 0.0137]], grad_fn=<TopkBackward0>)\n",
      "Top classes: tensor([[199, 198, 263, 197, 200]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:58:28.946926Z",
     "start_time": "2024-11-21T16:58:28.941608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#now labeling the outputs, we can read what the predictions are\n",
    "labels = model.config.id2label\n",
    "for i in range(num_predictions):\n",
    "    class_id = top_classes[0][i].item()\n",
    "    label = labels.get(class_id, \"Unknown\")\n",
    "    probability = top_probs[0][i].item()\n",
    "    print(f\"Class: {label}, Probability: {probability:.4f}\")"
   ],
   "id": "d07ce405e90091ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: standard schnauzer, Probability: 0.6593\n",
      "Class: giant schnauzer, Probability: 0.1369\n",
      "Class: Brabancon griffon, Probability: 0.0238\n",
      "Class: miniature schnauzer, Probability: 0.0233\n",
      "Class: Scotch terrier, Scottish terrier, Scottie, Probability: 0.0137\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we seek to get a sense of the model and data and make needed changes",
   "id": "544251cf56185cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:49:19.957965Z",
     "start_time": "2024-11-21T17:49:19.952872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#first we need to get a sense of the model\n",
    "print(model)\n",
    "#the model is made up of many 3x3 convolution layers that are then reduced and eventually coded to categorize them"
   ],
   "id": "1f10cf32dc61246d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2ForImageClassification(\n",
      "  (mobilenet_v2): MobileNetV2Model(\n",
      "    (conv_stem): MobileNetV2Stem(\n",
      "      (first_conv): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6()\n",
      "      )\n",
      "      (conv_3x3): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "        (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6()\n",
      "      )\n",
      "      (reduce_1x1): MobileNetV2ConvLayer(\n",
      "        (convolution): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (normalization): BatchNorm2d(16, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer): ModuleList(\n",
      "      (0): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(24, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(24, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
      "          (normalization): BatchNorm2d(144, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3-4): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(32, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
      "          (normalization): BatchNorm2d(192, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(64, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6-8): 3 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(64, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
      "          (normalization): BatchNorm2d(384, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10-11): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(96, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), groups=576, bias=False)\n",
      "          (normalization): BatchNorm2d(576, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(160, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13-14): 2 x MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(160, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): MobileNetV2InvertedResidual(\n",
      "        (expand_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (conv_3x3): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False)\n",
      "          (normalization): BatchNorm2d(960, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU6()\n",
      "        )\n",
      "        (reduce_1x1): MobileNetV2ConvLayer(\n",
      "          (convolution): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (normalization): BatchNorm2d(320, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_1x1): MobileNetV2ConvLayer(\n",
      "      (convolution): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(1280, eps=0.001, momentum=0.997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (classifier): Linear(in_features=1280, out_features=1001, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:16:53.848078Z",
     "start_time": "2024-11-21T17:16:53.316458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this block will help us get a sense of the data we are working with.\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pcuenq/oxford-pets\")\n",
    "print(dataset)\n",
    "# Now printing a single element\n",
    "print(dataset['train'][0]['image']['bytes'])"
   ],
   "id": "e596174a4276e842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['path', 'label', 'dog', 'image'],\n",
      "        num_rows: 7390\n",
      "    })\n",
      "})\n",
      "191\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:20:20.126334Z",
     "start_time": "2024-11-21T17:20:20.118862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#as the structure of the data is as raw bytes the easiest way to check the sizing is to change it into another format\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# we will look at 5 images to get a better sense of the data, as mobilenet v2 only accepts 224x224 images\n",
    "num_check = 5\n",
    "for i in range(num_check):\n",
    "    image_raw_test = dataset['train'][i]['image']['bytes']\n",
    "    image_conv = Image.open(BytesIO(image_raw_test))\n",
    "    print(f\"Image size: {image_conv.size}\")"
   ],
   "id": "fefb1bb9fc3fb520",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (345, 500)\n",
      "Image size: (290, 370)\n",
      "Image size: (333, 500)\n",
      "Image size: (500, 375)\n",
      "Image size: (416, 500)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:10:39.375653Z",
     "start_time": "2024-11-21T17:10:37.988216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#as the data is all different sizes we continue by resizing all images in our dataset to comply with the pre-trained model\n",
    "dataset_size = 7390\n",
    "def resize(datapoint):\n",
    "    image_raw = datapoint['image']['bytes']\n",
    "    image_workable = Image.open(BytesIO(image_raw)).convert('RGB')\n",
    "    image_workable = image_workable.resize((224, 224))\n",
    "    # Apply feature extractor to get the pixel values for the model\n",
    "    datapoint['image']['bytes'] = image_processor(images=image_workable, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return datapoint\n",
    "\n",
    "dataset_processed = dataset.map(resize, batched=False)"
   ],
   "id": "7f36f6135adaff14",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:37:38.334032Z",
     "start_time": "2024-11-21T17:37:38.325466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#noticing that the dataset is not split we do so making sure to keep a consistent random seed for reproducibility\n",
    "dataset_split = dataset_processed['train'].train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "# Access the splits\n",
    "train_data = dataset_split[\"train\"]\n",
    "test_data = dataset_split[\"test\"]\n",
    "print(train_data)\n",
    "print(test_data)"
   ],
   "id": "d8e9357d9343d552",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'label', 'dog', 'image'],\n",
      "    num_rows: 5912\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'label', 'dog', 'image'],\n",
      "    num_rows: 1478\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we begin the process of fine-tuning via transfer learning",
   "id": "54abf570421f29d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:41:31.223856Z",
     "start_time": "2024-11-21T17:41:31.211600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "#now we restructure the model to have 37 classes to match the pets dataset\n",
    "num_classes = 37\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)"
   ],
   "id": "c64f2213fdb1ad2c",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:54:18.327425Z",
     "start_time": "2024-11-21T17:54:18.317357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(\"hi\")\n",
    "        layer.reset_parameters()"
   ],
   "id": "2929420efa816039",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:34:36.815926Z",
     "start_time": "2024-11-21T18:34:36.805158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "saved_layers = 16\n",
    "for first_layer in model.children():\n",
    "    for second_layer in first_layer.children():\n",
    "        for third_layer in second_layer.children():\n",
    "            for fourth_layer in third_layer.children():\n",
    "                for name, conv_layer in fourth_layer.named_children():\n",
    "                    if(isinstance(conv_layer, nn.Conv2d)):\n",
    "                        if saved_layers == 0:\n",
    "                            conv_layer.reset_parameters()\n",
    "                        else:\n",
    "                            saved_layers -= 1"
   ],
   "id": "5cc84471aa3ab536",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:52:33.002964Z",
     "start_time": "2024-11-21T18:52:32.880588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Training phase\n",
    "    for image, labels in train_data:\n",
    "        image, labels = image.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output_model = model(image)\n",
    "        # Compute loss\n",
    "        loss = criterion(output_model, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output_model, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation phase (to check performance on validation set)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for inputs, labels in test_data:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * test_correct / test_total\n",
    "    print(f\"Validation Loss: {test_loss / len(test_data):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# After training, you can save the model\n",
    "torch.save(model.state_dict(), 'fine_tuned_model.pth')"
   ],
   "id": "4e637b1e1c48b5d0",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[85], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image, labels \u001B[38;5;129;01min\u001B[39;00m train_data:\n\u001B[0;32m     22\u001B[0m     image, labels \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     24\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 85
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
